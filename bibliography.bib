% Encoding: UTF-8

@Article{Timoshenko2017,
  author    = {Janis Timoshenko and Deyu Lu and Yuewei Lin and Anatoly I. Frenkel},
  journal   = {The Journal of Physical Chemistry Letters},
  title     = {Supervised Machine-Learning-Based Determination of Three-Dimensional Structure of Metallic Nanoparticles},
  year      = {2017},
  month     = {oct},
  number    = {20},
  pages     = {5091--5098},
  volume    = {8},
  doi       = {10.1021/acs.jpclett.7b02364},
  file      = {:Papers/Frenkel2017_SML_XANES.pdf:PDF},
  publisher = {American Chemical Society ({ACS})},
}

@Article{timoshenko2018neural,
  author    = {Timoshenko, Janis and Anspoks, Andris and Cintins, Arturs and Kuzmin, Alexei and Purans, Juris and Frenkel, Anatoly I},
  journal   = {Physical review letters},
  title     = {Neural network approach for characterizing structural transformations by X-ray absorption fine structure spectroscopy},
  year      = {2018},
  number    = {22},
  pages     = {225502},
  volume    = {120},
  file      = {:Papers/Frenkel2018_NN_EXAFS.pdf:PDF},
  publisher = {APS},
}

@Article{ng2011sparse,
  author  = {Ng, Andrew and others},
  journal = {CS294A Lecture notes},
  title   = {Sparse autoencoder},
  year    = {2011},
  number  = {2011},
  pages   = {1--19},
  volume  = {72},
  file    = {:Papers/sparseAutoencoder.pdf:PDF},
}

@Article{Bhowick2019,
  author        = {Debjani Bhowick and Deepak K. Gupta and Saumen Maiti and Uma Shankar},
  title         = {Stacked autoencoders based machine learning for noise reduction and signal reconstruction in geophysical data},
  year          = {2019},
  month         = jul,
  abstract      = {Autoencoders are neural network formulations where the input and output of the network are identical and the goal is to identify the hidden representation in the provided datasets. Generally, autoencoders project the data nonlinearly onto a lower dimensional hidden space, where the important features get highlighted and interpretation of the data becomes easier. Recent studies have shown that even in the presence of noise in the input data, autoencoders can be trained to reconstruct the noisefree component of the data from the reduced-dimensional hidden space. In this paper, we explore the application of autoencoders within the scope of denoising geophysical datasets using a data-driven methodology. The autoencoder formulation is discussed, and a stacked variant of deep autoencoders is proposed. The proposed method involves locally training the weights first using basic autoencoders, each comprising a single hidden layer. Using these initialized weights as starting points in the optimization model, the full autoencoder network is then trained in the second step. The applicability of denoising autoencoders has been demonstrated on a basic mathematical example and several geophysical examples. For all the cases, autoencoders are found to significantly reduce the noise in the input data.},
  archiveprefix = {arXiv},
  eprint        = {1907.03278},
  file          = {:Bhowick2019 - Stacked Autoencoders Based Machine Learning for Noise Reduction and Signal Reconstruction in Geophysical Data.pdf:PDF},
  keywords      = {eess.SP, physics.geo-ph},
  primaryclass  = {eess.SP},
}

@PhdThesis{gardenghi2012synchrotron,
  author = {Gardenghi, David Jeremiah and others},
  school = {Montana State University-Bozeman, College of Letters \& Science},
  title  = {Synchrotron radiation-based spectroscopic investigation of the electronic and geometric structures of iron-sulfur clusters, particles, and minerals},
  year   = {2012},
  file   = {Good summary of EXAFS and XANES:Papers/GardenghiD0512.pdf:PDF},
}

@Article{firstSynchrotronRadPaper,
  author    = {Elder, F. R. and Gurewitsch, A. M. and Langmuir, R. V. and Pollock, H. C.},
  journal   = {Phys. Rev.},
  title     = {Radiation from Electrons in a Synchrotron},
  year      = {1947},
  month     = {Jun},
  pages     = {829--830},
  volume    = {71},
  doi       = {10.1103/PhysRev.71.829.5},
  file      = {:Papers/synchrotron_radiation_source.pdf:PDF},
  issue     = {11},
  numpages  = {0},
  publisher = {American Physical Society},
  url       = {https://link.aps.org/doi/10.1103/PhysRev.71.829.5},
}

@Article{rehrXAFS2000review,
  author    = {Rehr, John J and Albers, Robert C},
  journal   = {Reviews of modern physics},
  title     = {Theoretical approaches to x-ray absorption fine structure},
  year      = {2000},
  number    = {3},
  pages     = {621},
  volume    = {72},
  file      = {:Papers/XAFS/rehr2000.pdf:PDF},
  publisher = {APS},
}

@Article{newville2014fundamentals,
  author    = {Newville, Matthew},
  journal   = {Reviews in Mineralogy and Geochemistry},
  title     = {Fundamentals of XAFS},
  year      = {2014},
  number    = {1},
  pages     = {33--74},
  volume    = {78},
  file      = {:Papers/XAFS/RIMG78_EXAFS_Newville.pdf:PDF},
  publisher = {Mineralogical Society of America},
}

@Article{fricke1920,
  author    = {Fricke, Hugo},
  journal   = {Physical Review},
  title     = {The K-characteristic absorption frequencies for the chemical elements magnesium to chromium},
  year      = {1920},
  number    = {3},
  pages     = {202},
  volume    = {16},
  publisher = {APS},
}

@Article{hertz1920ueber,
  author  = {Hertz, G},
  journal = {Zeitschrift fuer Physik},
  title   = {ueber die Absorptionsgrenzen in derL-Serie},
  year    = {1920},
  number  = {1},
  pages   = {19--25},
  volume  = {3},
}

@Article{skewnorm_Azzalini_1999,
  author    = {Azzalini, A. and Capitanio, A.},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  title     = {Statistical applications of the multivariate skew normal distribution},
  year      = {1999},
  issn      = {1467-9868},
  month     = {Aug},
  number    = {3},
  pages     = {579–602},
  volume    = {61},
  doi       = {10.1111/1467-9868.00194},
  publisher = {Wiley},
  url       = {http://dx.doi.org/10.1111/1467-9868.00194},
}

@Article{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  journal = {Nature Methods},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in Python}},
  year    = {2020},
  pages   = {261--272},
  volume  = {17},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@Software{thomas_a_caswell_2021_4475376,
  author    = {Thomas A Caswell and Michael Droettboom and Antony Lee and Elliott Sales de Andrade and John Hunter and Eric Firing and Tim Hoffmann and Jody Klymak and David Stansby and Nelle Varoquaux and Jens Hedegaard Nielsen and Benjamin Root and Ryan May and Phil Elson and Jouni K. Seppänen and Darren Dale and Jae-Joon Lee and Damon McDougall and Andrew Straw and Paul Hobson and Christoph Gohlke and Tony S Yu and Eric Ma and Adrien F. Vincent and hannah and Steven Silvester and Charlie Moad and Nikita Kniazev and Elan Ernest and Paul Ivanov},
  doi       = {10.5281/zenodo.4475376},
  month     = jan,
  publisher = {Zenodo},
  title     = {matplotlib/matplotlib: REL: v3.3.4},
  url       = {https://doi.org/10.5281/zenodo.4475376},
  version   = {v3.3.4},
  year      = {2021},
}

@InCollection{lin2020machine,
  author    = {Lin, Yuewei and Topsakal, Mehmet and Timoshenko, Janis and Lu, Deyu and Yoo, Shinjae and Frenkel, Anatoly I},
  booktitle = {HANDBOOK ON BIG DATA AND MACHINE LEARNING IN THE PHYSICAL SCIENCES: Volume 2. Advanced Analysis Solutions for Leading Experimental Techniques},
  publisher = {World Scientific},
  title     = {Machine-Learning Assisted Structure Determination of Metallic Nanoparticles: A Benchmark},
  year      = {2020},
  pages     = {127--140},
}

@Article{klementev2000xafs,
  author  = {Klementev, KV},
  journal = {arXiv preprint physics/0003086},
  title   = {XAFS spectroscopy. I. Extracting the fine structure from the absorption spectra},
  year    = {2000},
}

@Article{exafsbook,
  author = {Dr. Boon K. Teo},
  title  = {EXAFS: Basic Principles and Data Analysis},
  year   = {1986},
  doi    = {https://doi.org/10.1007/978-3-642-50031-2},
}

@InCollection{DW-diffraction,
  author    = {M. Rühle and M. Wilkens},
  booktitle = {Physical Metallurgy (Fourth Edition)},
  publisher = {North-Holland},
  title     = {Transmission Electron Microscopy},
  year      = {1996},
  address   = {Oxford},
  chapter   = {11},
  edition   = {Fourth Edition},
  editor    = {Robert W. Cahn and Peter Haasenae },
  isbn      = {978-0-444-89875-3},
  pages     = {1033-1113},
  doi       = {https://doi.org/10.1016/B978-044489875-3/50016-8},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780444898753500168},
}

@Article{schmidhuber2015deep,
  author    = {Schmidhuber, J{\"u}rgen},
  journal   = {Neural networks},
  title     = {Deep learning in neural networks: An overview},
  year      = {2015},
  pages     = {85--117},
  volume    = {61},
  doi       = {10.1016/j.neunet.2014.09.003},
  publisher = {Elsevier},
  url       = {https://arxiv.org/abs/1404.7828},
}

@Misc{tensorflow2015-whitepaper,
  author = {Martin~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  note   = {Software available from tensorflow.org},
  title  = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  year   = {2015},
  url    = {https://www.tensorflow.org/},
}

@InCollection{pytorch-paper,
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d' Alche-Buc and E. Fox and R. Garnett},
  pages     = {8024--8035},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
}

@Article{catalyst-strain-dependence,
  author    = {Uzio, Denis and Berhault, Gilles},
  journal   = {Catalysis Reviews},
  title     = {Factors governing the catalytic reactivity of metallic nanoparticles},
  year      = {2010},
  number    = {1},
  pages     = {106--131},
  volume    = {52},
  publisher = {Taylor \& Francis},
}

@Article{co-strain-effects,
  author    = {J{\o}rgensen, Mikkel and Gr{\"o}nbeck, Henrik},
  journal   = {Topics in Catalysis},
  title     = {Strain affects CO oxidation on metallic nanoparticles non-linearly},
  year      = {2019},
  number    = {7},
  pages     = {660--668},
  volume    = {62},
  publisher = {Springer},
}

@Article{gradient-descent-rev-article,
  author        = {Sebastian Ruder},
  journal       = {CoRR},
  title         = {An overview of gradient descent optimization algorithms},
  year          = {2016},
  volume        = {abs/1609.04747},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/Ruder16.bib},
  eprint        = {1609.04747},
  timestamp     = {Mon, 13 Aug 2018 16:48:10 +0200},
  url           = {http://arxiv.org/abs/1609.04747},
}

@Article{orig-ADAM-paper,
  author  = {Kingma, Diederik P and Ba, Jimmy},
  journal = {arXiv preprint arXiv:1412.6980},
  title   = {Adam: A method for stochastic optimization},
  year    = {2014},
}

@Book{Boas-mathmethods,
  author    = {Boas, Mary L},
  publisher = {Wiley},
  title     = {{Mathematical methods in the physical sciences; 3rd ed.}},
  year      = {2006},
  address   = {Hoboken, NJ},
  url       = {https://cds.cern.ch/record/913305},
}

@Article{cornell-convs,
  author  = {Ramin Zabih},
  journal = {Cornell University},
  title   = {CS1114 Section 6: Convolution},
  year    = {2013},
  month   = feb,
  note    = {Cornell University},
  groups  = {Cornell University},
  url     = {https://www.cs.cornell.edu/courses/cs1114/2013sp/sections/S06_convolution.pdf},
}

@Article{1dconv-NN-survey,
  author   = {Serkan Kiranyaz and Onur Avci and Osama Abdeljaber and Turker Ince and Moncef Gabbouj and Daniel J. Inman},
  journal  = {Mechanical Systems and Signal Processing},
  title    = {1D convolutional neural networks and applications: A survey},
  year     = {2021},
  issn     = {0888-3270},
  pages    = {107398},
  volume   = {151},
  abstract = {During the last decade, Convolutional Neural Networks (CNNs) have become the de facto standard for various Computer Vision and Machine Learning operations. CNNs are feed-forward Artificial Neural Networks (ANNs) with alternating convolutional and subsampling layers. Deep 2D CNNs with many hidden layers and millions of parameters have the ability to learn complex objects and patterns providing that they can be trained on a massive size visual database with ground-truth labels. With a proper training, this unique ability makes them the primary tool for various engineering applications for 2D signals such as images and video frames. Yet, this may not be a viable option in numerous applications over 1D signals especially when the training data is scarce or application specific. To address this issue, 1D CNNs have recently been proposed and immediately achieved the state-of-the-art performance levels in several applications such as personalized biomedical data classification and early diagnosis, structural health monitoring, anomaly detection and identification in power electronics and electrical motor fault detection. Another major advantage is that a real-time and low-cost hardware implementation is feasible due to the simple and compact configuration of 1D CNNs that perform only 1D convolutions (scalar multiplications and additions). This paper presents a comprehensive review of the general architecture and principals of 1D CNNs along with their major engineering applications, especially focused on the recent progress in this field. Their state-of-the-art performance is highlighted concluding with their unique properties. The benchmark datasets and the principal 1D CNN software used in those applications are also publicly shared in a dedicated website. While there has not been a paper on the review of 1D CNNs and its applications in the literature, this paper fulfills this gap.},
  doi      = {https://doi.org/10.1016/j.ymssp.2020.107398},
  keywords = {Artificial Neural Networks, Machine learning, Deep learning, Convolutional neural networks, Structural health monitoring, Condition monitoring, Arrhythmia detection and identification, Fault detection, Structural damage detection},
  url      = {https://www.sciencedirect.com/science/article/pii/S0888327020307846},
}

@InProceedings{local-translation-invariance,
  author    = {Kayhan, Osman Semih and Gemert, Jan C van},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {On translation invariance in cnns: Convolutional layers can exploit absolute spatial location},
  year      = {2020},
  pages     = {14274--14285},
}

@Misc{AdamW-orig,
  author        = {Ilya Loshchilov and Frank Hutter},
  howpublished  = {Conference Paper at ICLR 2019},
  title         = {Decoupled Weight Decay Regularization},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1711.05101},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/pdf/1711.05101.pdf},
}

@Article{grad-desc-limits,
  author    = {Wilson, D Randall and Martinez, Tony R},
  journal   = {Neural networks},
  title     = {The general inefficiency of batch training for gradient descent learning},
  year      = {2003},
  number    = {10},
  pages     = {1429--1451},
  volume    = {16},
  publisher = {Elsevier},
}

@Article{cauchy-orig-grad-descent,
  author  = {Cauchy, Augustin and others},
  journal = {Comp. Rend. Sci. Paris},
  title   = {M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des systemes d’{\'e}quations simultan{\'e}es},
  year    = {1847},
  number  = {1847},
  pages   = {536--538},
  volume  = {25},
}

@InProceedings{stoch-grad-desc-parallel,
  author       = {Zinkevich, Martin and Weimer, Markus and Smola, Alexander J and Li, Lihong},
  booktitle    = {NIPS},
  title        = {Parallelized stochastic gradient descent.},
  year         = {2010},
  number       = {1},
  organization = {Citeseer},
  pages        = {4},
  volume       = {4},
}

@Article{rumelhart1986learning,
  author    = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal   = {nature},
  title     = {Learning representations by back-propagating errors},
  year      = {1986},
  number    = {6088},
  pages     = {533--536},
  volume    = {323},
  publisher = {Nature Publishing Group},
}

@Article{qian1999momentum,
  author    = {Qian, Ning},
  journal   = {Neural networks},
  title     = {On the momentum term in gradient descent learning algorithms},
  year      = {1999},
  number    = {1},
  pages     = {145--151},
  volume    = {12},
  publisher = {Elsevier},
}

@InProceedings{improving-rprop,
  author       = {Igel, Christian and H{\"u}sken, Michael},
  booktitle    = {Proceedings of the second international ICSC symposium on neural computation (NC 2000)},
  title        = {Improving the Rprop learning algorithm},
  year         = {2000},
  organization = {Citeseer},
  pages        = {115--121},
  volume       = {2000},
}

@Article{adagrad,
  author  = {Duchi, John and Hazan, Elad and Singer, Yoram},
  journal = {Journal of machine learning research},
  title   = {Adaptive subgradient methods for online learning and stochastic optimization.},
  year    = {2011},
  number  = {7},
  volume  = {12},
}

@Article{2017marginal-adagrad,
  author  = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
  journal = {arXiv preprint arXiv:1705.08292},
  title   = {The marginal value of adaptive gradient methods in machine learning},
  year    = {2017},
}

@InProceedings{batch-norm-orig,
  author       = {Ioffe, Sergey and Szegedy, Christian},
  booktitle    = {International conference on machine learning},
  title        = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  year         = {2015},
  organization = {PMLR},
  pages        = {448--456},
}

@InProceedings{batch-norm-conference,
  author    = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  booktitle = {Proceedings of the 32nd international conference on neural information processing systems},
  title     = {How does batch normalization help optimization?},
  year      = {2018},
  pages     = {2488--2498},
}

@Article{deepCNNforImages,
  author    = {Rawat, Waseem and Wang, Zenghui},
  journal   = {Neural computation},
  title     = {Deep convolutional neural networks for image classification: A comprehensive review},
  year      = {2017},
  number    = {9},
  pages     = {2352--2449},
  volume    = {29},
  publisher = {MIT Press},
}

@Article{grad-desc-with-mom-orig,
  author    = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal   = {nature},
  title     = {Learning representations by back-propagating errors},
  year      = {1986},
  number    = {6088},
  pages     = {533--536},
  volume    = {323},
  publisher = {Nature Publishing Group},
}

@Thesis{williamsThesis,
  author = {Thaller, Jeremy},
  doi    = {10.5281/zenodo.4030517},
  school = {Williams College},
  title  = {Toward a Direct Measurement of Strain-Dependent Surface Stress in Soft Solids},
  year   = {2019},
}

@Book{statsTextbook,
  author    = {Larson, Ron and Farber, Betsy},
  publisher = {Pearson Education Canada},
  title     = {Elementary statistics},
  year      = {2019},
}

@InProceedings{whybatchnorm1,
  author    = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  booktitle = {Proceedings of the 32nd international conference on neural information processing systems},
  title     = {How does batch normalization help optimization?},
  year      = {2018},
  pages     = {2488--2498},
}

@InProceedings{whybatchnorm2,
  author       = {Kohler, Jonas and Daneshmand, Hadi and Lucchi, Aurelien and Hofmann, Thomas and Zhou, Ming and Neymeyr, Klaus},
  booktitle    = {The 22nd International Conference on Artificial Intelligence and Statistics},
  title        = {Exponential convergence rates for batch normalization: The power of length-direction decoupling in non-convex optimization},
  year         = {2019},
  organization = {PMLR},
  pages        = {806--815},
}

@Article{whybatchnorm3,
  author  = {Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz},
  journal = {arXiv preprint arXiv:1902.08129},
  title   = {A mean field theory of batch normalization},
  year    = {2019},
}

@Article{conv-dropout-layers,
  author  = {Cai, Shaofeng and Shu, Yao and Chen, Gang and Ooi, Beng Chin and Wang, Wei and Zhang, Meihui},
  journal = {arXiv preprint arXiv:1904.03392},
  title   = {Effective and efficient dropout for deep convolutional neural networks},
  year    = {2019},
}

@Article{conv-dropout-layers2,
  author    = {Wu, Haibing and Gu, Xiaodong},
  journal   = {Neural Networks},
  title     = {Towards dropout training for convolutional neural networks},
  year      = {2015},
  pages     = {1--10},
  volume    = {71},
  publisher = {Elsevier},
}

@Article{gold-lattice-const,
  author    = {Boswell, Frank William Charles},
  journal   = {Proceedings of the Physical Society. Section A},
  title     = {Precise determination of lattice constants by electron diffraction and variations in the lattice constants of very small crystallites},
  year      = {1951},
  number    = {5},
  pages     = {465},
  volume    = {64},
  publisher = {IOP Publishing},
}

@Comment{jabref-meta: databaseType:bibtex;}
